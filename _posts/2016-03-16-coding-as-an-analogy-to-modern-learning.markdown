---
layout: post
title:  "First Post" (NEEDS EDIT AND FORMATING)
date:   2016-03-15 15:26:36 -0500
categories: coding, learning
---

So this is my first post here, please bear with me. I’m currently making the very real life altering attempt to learn coding (via Bloc’s software engineering track) to learn enough programming skills to actually be able to be notably employable in today’s tech-driven economy. In the future, hopefully I’ll be posting more detailed technical analysis of various tools, functions, products, and what have you, that will actually speak with a little more technical authority and detail, but right now I wanted to start with a larger conceptual introduction that notes my first feelings and impressions about the current nature of coding myself.
It may have taken a few weeks for me to realize it, but what struck me first, and still lingers in my mind is the nature of how knowledge is acquired and problems are solved in this field. In short, that few people (if anyone) actually knows ‘everything’ (or that much,) but rather the highly skilled people know how to approach a problem, what information to find, how to find it, and then how process it and implement it.
See after years of being fairly educated (I graduated from a good school with a fairly useless liberal-arts degree,) but with no particular set of skills that made me employable, the few jobs I was able to find left me riddled with imposter syndrome(LINK). I knew I was capable of plenty of things, but I didn’t really feel as though I had the sort of knowledge I could rattle of the top of my head with authority that would make me sound capable, much less qualified. I’d really hoped in learning to program, I’d have found a solution to this problem; that I’d be able to easily demonstrate a real grounded understanding of concepts that I could clearly demonstrate to an outsider (or perspective employer) and be able to look and feel like I really knew what I was doing. Well as it turns out, I couldn’t have been more wrong.
My schooling has started with Ruby and specifically, mostly Ruby on Rails. Now Ruby, I can follow, and I can see how to use a (much of) it’s functionality logically (outside of a few things like regex and a handful of other aspects I don’t have a great grasp on yet,) but it’s learning to use Rails that I really start to feel like an imposter again. Sure in effect, I understand it technically, but after about a month I actually feel a lot like a new language speaker: It’s easy enough to read and process, but actually attempting to write or speak it is quite difficult.  That might be par for the course in learning to code (I haven’t done much before this so I wouldn’t really know,) but it’s how I’ve found people solve problems that is really surprising to me.
I can’t tell you how many times people, (mentors, online tutorials, friends with legitimate coding jobs,) basically tell me some variety of the following:
“Did you search for X?”
“Have you checked for that question on StackOverflow?”
“Did you read the Api doc?”
And on it goes…
See I’m starting to learn that the notion of mastery, or “really knowing what you’re doing” is kind of an illusion. That, particularly with Rails (which is HUGE, and full of specific functionality and add-ons that have been pre-coded to make our lives easier and our apps more elegant,) that there’s actually “knowing everything” weather or not possible, is impractical and inefficient. In the real world - what’s the end goal? To make your code work, to have functionality, fast and error free - right? So in the end, what’s the difference how you get there? The code itself doesn’t really look different if you somehow “knew it properly” vs “found the answer in forum.” I even had one developer explain to me in one of my moments of doubt, “don’t worry, I spend half my workday looking things up on google.” Sure it feels like “cheating,” but were not in grade school anymore.
Which brings me to my next point, in the days of google and smart phones, where everyone effectively already has all the knowledge in the world at their fingertips - isn’t this how everything works? Or at least, isn’t this how everything should work? Isn’t this how knowledge can be most efficiently acquired these days? Isn’t it a better use of our time to work on figuring out how to solve problems effectively by efficiently using the massive collection of incredibly powerful tools available to us? Otherwise why have computer technology in the first place?
I think a lot to the doubt over this method, the sense that it’s somehow “cheating” or “not real,” harkens back to the way my generation, (and probably others) were educated in primary and secondary school, in way that now seems quite antiquated. I recall early in high school (and probably the end of middle school as well,) when doing a research paper, the teachers would either limit (or completely forbid) the number of online sources you could use and cite. Now granted, this was 15 years ago or more, so the internet wasn’t quite what it was now in terms of available credible information, and general ubiquity, but all the same it wasn’t the tabloid, slanderous, anti-informative-joke my teachers thought it was either. This fight against the internet, technology, and ultimately this approach to learning, in retrospect seemed to be about two things. First, the obvious one, is that our teachers were old, and didn’t really understand the internet. The second, which is related, reflects the notion of “cheating,” or “not working hard enough.” In short, it was easier for us to find information in all respects, by teaching online, rather than going to a physical library and trying to use whatever antiquated system they had to look up books, periodicals, and encloypedias, that were often 40 years old, (when they were even where they were supposed to be.) It was well intended, they wanted us actually to be invested in our work, and they also were afraid that we weren’t going to learn how to legitimately research in the way they had come to understand it, and instead they feared we would be dependent on a crutch.
The problem with this theory, was the crutch was quickly becoming a superior means of transportation than their horse-and-buggy approach, or just using our own legs. I think it’s safe to say that when kids are in library now, they’re only there to use the internet away from their parents. I don’t think there’s much of any research being done without use of the internet, and almost all knowledge that hasn’t been digitized and uploaded, will be by the time you finish reading this. I can  still remember in something like third grade being forced to learn to write in cursive, (which was probably my least favorite subject.) I had computers early on my life, so I felt confident to challenge my teacher on the grounds that cursive wasn’t important to learn (because recall people used to use it because it was faster than traditional handwriting) because it was still slower than typing, and I pointed out that if we wanted our writing to look fancy like that, we could would be able get a font to do it for us. Her argument was basically “No we won’t, get back to work,” and to this day as I collect e-signatures for forms for publications to a prestigious scholarly journal via email, and scribble unintelligible signatures with my fingertip on iPads for contracts and transactions, I feel pretty damn vindicated.
But the larger point here is the notion of “real work,” both in the cases of cursive and going to the library, the teachers were to some degree actually right, we were trying to get out of doing it. But the problem comes back to the definition of work, and what purpose it serves. We want to be invested in our work, but our investment doesn’t come from unnecessarily arduous inefficient labor, and our lack of investment didn’t originate from the availability of the internet, but rather from the mostly meaningless and sophomoric nature of completing a trite high-school research assignments. I spent plenty of time in later life using all the “cheat/crutch” tools to solve real problems, no part of which felt “easy.”  The recent foray into web-development is no exception.
 It wasn’t until my senior year of high school that I actually learned what real knowledge was really for. In a final exam for my AP Euro History course I was actually asked to make arguments about various components of nationalism, colonialism, as well as the industrial revolution. Certainly I needed various “facts” at my disposal and needed to be able to reference specific details, but specific names and dates were well outweighed by having a greater conceptual understanding of these ideas and being to relate them meaningfully to actually make a real “argument” about history. When I commented to my teacher about enjoying this challenge and finding it more fulfilling than a traditional “multiple choice/fill in the blank/answer this specific question” test, struggling to define what it was we were now doing here instead, he replied with something like “What? You mean talk about concepts? Yeah, this is what history actually is.” He was right. In fact later, while in college in a lecture about american cultural history from probably the smartest professor I’ve ever met, I watched him pull out an old dusty history textbook and say something to the effect of “This is a history book,” before opening it to a random page and reading from it “in 1898 the vice president of america was…WHO CARES?” and chucking it into a trash can. The point he was really making, and by some extension I’m making here, is specific detailed knowledge is already out there. It’s not going anywhere. We can access it when we needed, and trying to grasp all of it is futile and ineffective, and misses the larger point of what this knowledge aims to provide us. In short, while knowing who FDR was and being able to date and name some his most famous acts isn’t particularly valuable when that information can be recalled from a device in your pocket in less than minute, being able to conceptually understand and debate the merits and actual meaning of the New Deal, and make your own arguments about them, is invaluable, particularly as it relates to contemporary issues.
With this in mind it’s perhaps time for a revaluation of our basic education methods. We ask young students to learn and memorize massive collections of facts, but all the while knowing that in our day to day life, they don’t particularly serve us. While of course, having some specific information on hand for easy recall can be invaluable and make one more efficient, these things are generally learned and stored, by regular use and necessity, rather than as an exercise for it’s own sake. It may be time to start teaching young people, not information itself, but how to efficiently find it, how to understand it, how to evaluate it for accuracy and usefulness, and ultimately how to connect it with everything around them in order to actually solve problems.
