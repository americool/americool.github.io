---
layout: post
title:  "Coding & Learning: 'Knowledge' is Overrated"
date:   2016-03-15 15:26:36 -0500
categories: coding, learning
---

So this is my first post here, please bear with me. I’m currently making the very real life altering attempt to learn coding (via Bloc’s software engineering track.) The idea is when this is all said and done, I'll actually be able to be notably employable in today’s tech-driven economy, but I'll believe it when it happens. In the future, I hope to post more detailed technical analysis of various tools, functions, and products that will actually speak with a little more technical authority and detail, but right now I wanted to start with a larger conceptual introduction that notes my first feelings and impressions about the current nature of working as a coder.

It may have taken a few weeks for me to realize it, but first struck me (and I still wrestle with) is how one acquires and uses knowledge to solve problems in this field. In short, it would seem that few people (if anyone) actually know ‘everything’ (or even all that much,) but rather that highly skilled people know how to approach a problem, what information they need to find, how to find it easily, and then how process and implement said information.

After years of being fairly educated (I graduated from a good school with a fairly useless liberal-arts degree,) but with no particular set of skills that made me employable, the few jobs I was able to find left me riddled with [imposter syndrome][imposter_syndrome]. I knew I was capable of plenty of things, but I didn’t really feel as though I had the sort of knowledge I could rattle of the top of my head with the authority that would make me sound capable, or at least qualified. I had really hoped in learning to program, I’d have found a solution to this problem. I figured with some work, I’d be able to easily demonstrate a real grounded understanding of the field via terminology and facts that I could clearly demonstrate to an outsider or perspective employer. Well as it turns out, I couldn’t have been more wrong.

My schooling has started with Ruby and mostly working with Ruby on Rails. Now Ruby, I can mostly follow. I can see how to use (much of) it’s functionality logically (outside of a few things like regex and a handful of other aspects I don’t have a great grasp on yet.) However it’s learning to use Rails that I really start to feel like an imposter again. Sure in effect, I understand it technically, but after about a month I actually feel a lot like a new language speaker: It’s easy enough for me to read and process, but actually attempting to write or speak it is quite difficult.  That might be par for the course in learning to code (I haven’t done much before this so I wouldn’t really know,) but what has been really surprising to me, is how I've been instructed to solve a problem when I find myself stuck.

I can’t tell you how many times people, (and I mean experienced professionals in the field,) have basically told me some variety of the following:

- “Did you search for X?”
- “Have you checked for that question on StackOverflow?”
- “Did you read the Api doc?”

And on it goes…

See I’m starting to learn that the notion of mastery, or “really knowing what you’re doing” is kind of an illusion. That, particularly with Rails, (which is _HUGE_ and full of specific functionality and add-ons that have been pre-coded to make our lives easier and our apps more elegant,) actually *“knowing everything”* - even if possible, is impractical and inefficient. In the real world - what’s the end goal? To make your code work. To have functionality - fast and error free - *right*? So in the end, what’s the difference how you get there? The code itself doesn’t really look different if you somehow “knew it properly” vs “found the answer in forum.” I even had one developer explain to me in one of my many moments of doubt, *“don’t worry, I spend half my workday looking things up on google.”* Sure it feels like “cheating,” but were not in grade school anymore.

Which brings me to my next point, in the days of google and smart phones, where everyone effectively already has all the knowledge in the world at their fingertips - isn’t this how everything works? Or at least, isn’t this how everything should work? Isn’t this how knowledge can be most efficiently acquired these days? Isn’t it a better use of our time to work on figuring out how to solve problems effectively by efficiently using the massive collection of incredibly powerful tools available to us? Otherwise why have computer technology in the first place?

I think a lot of this doubt -- the sense that it’s somehow “cheating” or “not real,” harkens back to the way my generation, (and probably others) were educated in primary and secondary school. Which in retrospect now feels quite antiquated. I recall early in high school (and probably the end of middle school as well,) when doing a research paper, the teachers would either limit (or completely forbid) the number of online sources you could use and cite. Now granted, this was 15 years ago or more, so the internet didn't quite  have the same general ubiquity in terms of available and credible information, but it still wasn't the tabloid-esque, slanderous, anti-informative-joke my teachers thought it was either. Looking back, this fight against the internet, technology, and ultimately a different approach to learning, seems to be about two things: First, the obvious one, is that our teachers were older, and didn’t really understand the internet. The second, which is related, reflects the notion of “cheating,” or “not working hard enough.” In short, it was easier for us to find information by searching online rather than going to a physical library and trying to use whatever messy system they had to look up dusty books and periodicals, which were often decades old, misplaced, and mislabeled. It was well intended, they wanted us actually to be invested in our work, and they also were afraid that we weren’t going to learn how to legitimately do research in the way they had come to understand it, and they (perhaps rightly) feared we would be dependent on a crutch.

The problem with their theory, was that the crutch was quickly becoming a superior means of transportation, and using ones own legs was going the way of the horse-and-buggy. I'd suspect when kids are in library now, they’re only there to use the internet a safe distance away from their parents. At this point its hard to imagine doing any research being done without serious support from the internet and  whatever knowledge that hasn’t been digitized and uploaded will be by the time you finish reading this.

I can still remember in something like third grade being forced to learn to write in cursive, (which was probably my least favorite subject.) I had been exposed to computers early on my life, so I felt confident to challenge my teacher on the grounds that cursive wasn’t important to learn. If you can recall (*and I don't really*,) people used to use cursive because it was faster than traditional handwriting. However, as nine-year old me pointed out, it was still slower than touch-typing, and if we wanted our writing to look *'fancy'* like that, we could would be able get a font to do it for us. Kinda obnoxious, but seemed reasonable to me. My teacher's argument was the very nuanced commentary of *“No, that won't happen, get back to work.”*  Nowadays, when I collect e-signatures on forms for publications to a prestigious scholarly journal and scribble unintelligible signatures with my fingertip on iPads for contracts and transactions, I feel somewhat vindicated.

But the larger point here is the notion of **“real work.”** In both cases of cursive and going to the library, the teachers were to some degree actually right, we were trying to get out of doing more labor. But the problem still comes back to the definition of what work really is and what purpose it serves. Of course we want to be invested in our work, but our investment doesn’t come from unnecessarily arduous inefficient labor, and our lack of investment didn’t originate from the availability of the internet, but rather from the mostly meaningless and sophomoric nature of completing a trite high-school research assignment. Solving a *real* or *serious* problem, will always pose a challenge, no matter what tools you have available. I spent plenty of time in later life using all the “cheat/crutch” tools to solve real problems, yet no part of which felt “easy.”  This recent foray into web-development is no exception.

With this in mind it’s perhaps time for a revaluation of our basic education methods. We ask young students to learn and memorize massive collections of facts, but all the while knowing that in our day to day life, they don’t particularly serve us. We give "tests" and tell students to put away their textbooks and not talk to each other, but in what part of real life in the workforce are you unable to look up information or consult with your coworkers? While of course, having some specific information on hand for easy recall can be invaluable and make one more efficient, these things are generally learned and stored by regular use and necessity rather than as an exercise for it’s own sake. With this in mind it may be time to start teaching young people with less focus on specific information and more focus on how efficiently find said information. Perhaps even more important, is that we also teach them how to understand it, how to evaluate it for accuracy and usefulness, and ultimately how to connect it with everything around them in order to actually solve real problems.

[imposter_syndrome]: https://en.wikipedia.org/wiki/Impostor_syndrome
